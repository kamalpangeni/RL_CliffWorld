{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with epsilon-greedy action selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate how Q-learning behaves on Cliff World (Page 132 of textbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_glue import RLGlue\n",
    "import agent\n",
    "import cliffworld_env\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from matplotlib.colors import hsv_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':15})\n",
    "plt.rcParams.update({'figure.figsize':[10,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(agent.BaseAgent):\n",
    "    def agent_init(self, agent_init_info):\n",
    "        '''\n",
    "        Setup for agent called when the experiment first starts\n",
    "        '''\n",
    "        \n",
    "        # store the parameters provided in agent_init_info.\n",
    "        self.num_actions = agent_init_info['num_actions']\n",
    "        self.num_states = agent_init_info['num_states']\n",
    "        self.epsilon = agent_init_info['epsilon']\n",
    "        self.step_size = agent_init_info['step_size']\n",
    "        self.discount = agent_init_info['discount']\n",
    "        self.rand_generator = np.random.RandomState(agent_info['seed'])\n",
    "        \n",
    "        #create an array for action-value estimates and initialize to zero\n",
    "        self.q = np.zeros((self.num_states,self.num_actions))\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        '''\n",
    "        The first method called when the episode starts, called after the\n",
    "        environment starts.\n",
    "        '''\n",
    "        # Choose action using epsilon greedy.\n",
    "        \n",
    "        current_q = self.q[state,:]\n",
    "        if self.rand_generator.rand()<self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        '''\n",
    "        A step taken by the agent\n",
    "        '''\n",
    "        \n",
    "        #choose action using epsilon greedy\n",
    "        \n",
    "        current_q = self.q[state,:]\n",
    "        \n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        #perform an update\n",
    "        self.q[self.prev_state, self.prev_action] += self.step_size*(reward+self.discount*np.max(current_q)\n",
    "                                                                     -self.q[self.prev_state,self.prev_action])\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        '''\n",
    "        Run when the agent terminates.\n",
    "        '''\n",
    "        self.q[self.prev_state,self.prev_action] += self.step_size*(reward-self.q[self.prev_state,self.prev_action])\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        '''\n",
    "        argmax with random tie-breaking\n",
    "        '''\n",
    "        \n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "        \n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i]>top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "        return self.rand_generator.choice(ties)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:33<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "env = cliffworld_env.Environment\n",
    "all_reward_sums = [] #Contains sum of rewards during episodes\n",
    "all_state_visits = [] # Contains state visit counts during the last 10 episodes\n",
    "agent_info = {\"num_actions\":4,\"num_states\":48,\"epsilon\":0.1,\"step_size\":0.5,\"discount\":1.0}\n",
    "env_info = {}\n",
    "num_runs = 100\n",
    "num_episodes = 500\n",
    "rsa_first_episode = [] # reward state action for first episode\n",
    "rsa_second_episode = [] # reward state action for first episode\n",
    "rsa_last_episode = [] # reward state action for last episode\n",
    "for run in tqdm(range(num_runs)):\n",
    "    agent_info['seed'] = run\n",
    "    rl_glue = RLGlue(env,QLearningAgent)\n",
    "    rl_glue.rl_init(agent_info,env_info)\n",
    "    reward_sums = []\n",
    "    state_visits = np.zeros(48)\n",
    "    for episode in range(num_episodes):\n",
    "        if run==0 and episode==0:\n",
    "            state, action = rl_glue.rl_start()\n",
    "            rsa_first_episode.append([0.0,state,action]) # for visualization\n",
    "            state_visits[state] +=1\n",
    "            is_terminal = False\n",
    "            while not is_terminal:\n",
    "                reward, state, action, is_terminal = rl_glue.rl_step()\n",
    "                rsa_first_episode.append([reward,state,action]) # for visualization\n",
    "            state_visits[state] += 1\n",
    "        elif run==0 and episode==1:\n",
    "            state, action = rl_glue.rl_start()\n",
    "            rsa_second_episode.append([0.0,state,action]) # for visualization\n",
    "            state_visits[state] +=1\n",
    "            is_terminal = False\n",
    "            while not is_terminal:\n",
    "                reward, state, action, is_terminal = rl_glue.rl_step()\n",
    "                rsa_second_episode.append([reward,state,action]) # for visualization\n",
    "            state_visits[state] += 1\n",
    "        elif episode>2 and episode < num_episodes - 10:\n",
    "            rl_glue.rl_episode(0)\n",
    "        else:\n",
    "            # Runs an episode while keeping track of visited states\n",
    "            state, action = rl_glue.rl_start()\n",
    "            if run==num_runs-1 and episode == num_episodes-9:\n",
    "                    rsa_last_episode.append([0.0,state,action]) # for visualization\n",
    "            state_visits[state] +=1\n",
    "            is_terminal = False\n",
    "            while not is_terminal:\n",
    "                reward, state, action, is_terminal = rl_glue.rl_step()\n",
    "                if run==num_runs-1 and episode == num_episodes-1:\n",
    "                    rsa_last_episode.append([reward,state,action]) # for visualization\n",
    "                state_visits[state] += 1\n",
    "        reward_sums.append(rl_glue.rl_return())\n",
    "    all_reward_sums.append(reward_sums)\n",
    "    all_state_visits.append(state_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.mean(all_reward_sums,axis=0))\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Sum of \\n rewards \\n during \\n episode', rotation=0, labelpad = 40)\n",
    "plt.xlim(0,500)\n",
    "plt.ylim(-100,0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://github.com/lgvaz/blog/blob/master/rl_intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_range(values, vmin=0, vmax=1):\n",
    "    start_zero = values - np.min(values)\n",
    "    return (start_zero / (np.max(start_zero) + 1e-7)) * (vmax - vmin) + vmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    terrain_color = dict(normal=[127/360, 0, 96/100],\n",
    "                         objective=[26/360, 100/100, 100/100],\n",
    "                         cliff=[247/360, 92/100, 70/100],\n",
    "                         player=[344/360, 93/100, 100/100])\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.player = None\n",
    "        self._create_grid()  \n",
    "        self._draw_grid()\n",
    "        \n",
    "    def _create_grid(self, initial_grid=None):\n",
    "        self.grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "        self._add_objectives(self.grid)\n",
    "        \n",
    "    def _add_objectives(self, grid):\n",
    "        grid[0, 1:11] = self.terrain_color['cliff']\n",
    "        grid[0, 11] = self.terrain_color['objective']\n",
    "        \n",
    "    def _draw_grid(self):\n",
    "        self.fig, self.ax = plt.subplots(figsize=(12, 4))\n",
    "        self.ax.grid(which='minor')       \n",
    "        self.q_texts = [self.ax.text(*self._id_to_position(i)[::-1], '0',\n",
    "                                     fontsize=11, verticalalignment='center', \n",
    "                                     horizontalalignment='center') for i in range(12 * 4)]     \n",
    "         \n",
    "        self.im = self.ax.imshow(hsv_to_rgb(self.grid), cmap='terrain',\n",
    "                                 interpolation='nearest', vmin=0, vmax=1)        \n",
    "        self.ax.set_xticks(np.arange(12))\n",
    "        self.ax.set_xticks(np.arange(12) - 0.5, minor=True)\n",
    "        self.ax.set_yticks(np.arange(4))\n",
    "        self.ax.set_yticks(np.arange(4) - 0.5, minor=True)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (0, 0)        \n",
    "        return self._position_to_id(self.player)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Possible actions\n",
    "        if action == 0 : #right  min(new_state[1]+1,self.cols-1)\n",
    "            self.player = (self.player[0],min(self.player[1]+1, 11))\n",
    "        if action == 1 : # down max(new_state[0]-1,0)\n",
    "            self.player = (max(self.player[0] - 1, 0),self.player[1])\n",
    "        if action == 2 : #left max(new_state[1]-1,0)\n",
    "            self.player = (self.player[0],max(self.player[1]-1, 0))\n",
    "        if action == 3: #up min(new_state[0]+1, self.rows-1)\n",
    "            self.player = (min(self.player[0]+1, 3),self.player[1])\n",
    "            \n",
    "        # Rules\n",
    "        if all(self.grid[self.player] == self.terrain_color['cliff']):\n",
    "            reward = -100\n",
    "            done = True\n",
    "        elif all(self.grid[self.player] == self.terrain_color['objective']):\n",
    "            reward = 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "            \n",
    "        return self._position_to_id(self.player), reward, done\n",
    "    \n",
    "    def _position_to_id(self, pos):\n",
    "        ''' Maps a position in x,y coordinates to a unique ID '''\n",
    "        return pos[0] * 12 + pos[1]\n",
    "    \n",
    "    def _id_to_position(self, idx):\n",
    "        return (idx // 12), (idx % 12)\n",
    "        \n",
    "    def render(self, q_values=None, action=None, max_q=False, colorize_q=False):\n",
    "        assert self.player is not None, 'You first need to call .reset()'  \n",
    "        \n",
    "        if colorize_q:\n",
    "            assert q_values is not None, 'q_values must not be None for using colorize_q'            \n",
    "            grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "            values = change_range(np.max(q_values, -1)).reshape(4, 12)\n",
    "            grid[:, :, 1] = values\n",
    "            self._add_objectives(grid)\n",
    "        else:            \n",
    "            grid = self.grid.copy()\n",
    "            \n",
    "        grid[self.player] = self.terrain_color['player']       \n",
    "        self.im.set_data(hsv_to_rgb(grid))\n",
    "               \n",
    "        if q_values is not None:\n",
    "            xs = np.repeat(np.arange(12), 4)\n",
    "            ys = np.tile(np.arange(4), 12)  \n",
    "            \n",
    "            for i, text in enumerate(self.q_texts):\n",
    "                if max_q:\n",
    "                    q = max(q_values[i])    \n",
    "                    txt = '{:.2f}'.format(q)\n",
    "                    text.set_text(txt)\n",
    "                else:                \n",
    "                    actions = ['R', 'D', 'L', 'U']\n",
    "                    txt = '\\n'.join(['{}: {:.2f}'.format(k, q) for k, q in zip(actions, q_values[i])])\n",
    "                    text.set_text(txt)\n",
    "                \n",
    "        if action is not None:\n",
    "            self.ax.set_title(action, color='r', weight='bold', fontsize=32)\n",
    "\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['RIGHT', 'DOWN', 'LEFT', 'UP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(q_values,rsa):\n",
    "    env = GridWorld()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    i=0\n",
    "    for _,state,action in rsa[:-1]:    \n",
    "        # Select action\n",
    "        env.step(action)\n",
    "        if rsa[i+1][1]==0 and i>0:\n",
    "            state = env.reset()\n",
    "        i+=1\n",
    "        env.render(q_values=q_values, action=actions[action], colorize_q=False,max_q=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(None,rsa_first_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(None,rsa_second_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = rl_glue.agent.q\n",
    "play(q_values, rsa_last_episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
